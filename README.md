# Double DQN with CartPole-v1 compare with DQN

Key Modifications:
Added target_model and periodically copied weights from model using set_weights.
Used model to choose actions (via np.argmax), but used target_model to calculate Q-values for the next state (np.argmax(model.predict(next_state)[0])).
Updated target_model every 10 episodes (update_target_frequency) to prevent Q-value overestimation and enhance learning stability.
With these changes, we have implemented Double DQN! ğŸš€

DDQN results: Average Reward: 9.50, Max Reward: 11.0, Min Reward: 8.0

Define replay

    def replay(batch_size):
        global epsilon
        minibatch = random.sample(memory, batch_size)
        for state, action, reward, next_state, done in minibatch:
            target = reward
            if not done:
                # **Double DQN**
                best_action = np.argmax(model.predict(next_state)[0]) # é¸æ“‡æœ€å„ªå‹•ä½œ (ç”± model æ±ºå®š)
                target = reward + gamma * target_model.predict(next_state)[0][best_action]  # ç”¨ target_model è¨ˆç®— Q å€¼
        
            target_f = model.predict(state)  # å–å¾—ç•¶å‰ Q å€¼
            target_f[0][action] = target #target_f[0] å–å‡ºæ•´å€‹ Q å€¼é™£åˆ—
            model.fit(state, target_f, epochs=1, verbose=0)
        if epsilon > epsilon_min:
            epsilon *= epsilon_decay

            # Training loop

Train DDQN

    episodes = 50  # More episodes to ensure sufficient training
    batch_size = 32  # Mini-batch size for replay training
    gamma = 0.95  # Discount factor for future rewards
    update_target_frequency = 10  # æ¯ 10 å›åˆæ›´æ–°ä¸€æ¬¡ target model
     
    for e in range(episodes):
        state = env.reset()
        if isinstance(state, tuple):  # Handle tuple output
            state = state[0]
        #å¦‚æœ state æ˜¯ tupleï¼Œå‰‡ åªå– state[0] ä½œç‚ºç‹€æ…‹å€¼ã€‚
        #é€™æ˜¯ç‚ºäº†é©é…æ–°ç‰ˆ Gymnasium çš„ env.reset()ï¼Œå› ç‚ºèˆŠç‰ˆ Gym åªè¿”å› stateï¼Œè€Œæ–°ç‰ˆå¯èƒ½æœƒè¿”å› (state, info)ã€‚
    
        state = np.reshape(state, [1, state_size])
     
        for time in range(200):  # Max steps per episode
            # Choose action using epsilon-greedy policy
            action = act(state)
     
            # Perform action in the environment ä¸åŒç‰ˆæœ¬çš„ Gym å’Œ Gymnasium å¯èƒ½è¿”å› 4 å€‹å€¼æˆ– 5 å€‹å€¼
            result = env.step(action)
            if len(result) == 4:  # Handle 4-value output
                next_state, reward, done, _ = result
            else:  # Handle 5-value output
                next_state, reward, done, _, _ = result
     
            if isinstance(next_state, tuple):  # Handle tuple next_state
                next_state = next_state[0]
            next_state = np.reshape(next_state, [1, state_size])
     
            # Store experience in memory
            remember(state, action, reward, next_state, done)
     
            # Update state
            state = next_state
     
            if done:  # If episode ends
                print(f"Episode: {e+1}/{episodes}, Score: {time}, Epsilon: {epsilon:.2}")
                break
     
        # Train the model using replay memory
        if len(memory) > batch_size:
            replay(batch_size)
    
        # **æ¯ N å›åˆæ›´æ–°ä¸€æ¬¡ target model**
        if e % update_target_frequency == 0:
            target_model.set_weights(model.get_weights())
     
    env.close()
  
